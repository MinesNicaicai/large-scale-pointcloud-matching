{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate RPMNet. Also contains functionality to compute evaluation metrics given transforms\n",
    "\n",
    "Example Usages:\n",
    "    1. Evaluate RPMNet\n",
    "        python eval.py --noise_type crop --resume [path-to-model.pth]\n",
    "\n",
    "    2. Evaluate precomputed transforms (.npy file containing np.array of size (B, 3, 4) or (B, n_iter, 3, 4))\n",
    "        python eval.py --noise_type crop --transform_file [path-to-transforms.npy]\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import open3d  # Need to import before torch\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from arguments import rpmnet_eval_arguments\n",
    "from common.misc import prepare_logger\n",
    "from common.torch import dict_all_to_device, CheckPointManager, to_numpy\n",
    "from common.math import se3\n",
    "from common.math_torch import se3\n",
    "from common.math.so3 import dcm2euler\n",
    "from data_loader.datasets import get_test_datasets\n",
    "import models.rpmnet\n",
    "\n",
    "\n",
    "def compute_metrics(data: Dict, pred_transforms) -> Dict:\n",
    "    \"\"\"Compute metrics required in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def square_distance(src, dst):\n",
    "        return torch.sum((src[:, :, None, :] - dst[:, None, :, :]) ** 2, dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_transforms = pred_transforms\n",
    "        gt_transforms = data['transform_gt']\n",
    "        points_src = data['points_src'][..., :3]\n",
    "        points_ref = data['points_ref'][..., :3]\n",
    "        points_raw = data['points_raw'][..., :3]\n",
    "\n",
    "        # Euler angles, Individual translation errors (Deep Closest Point convention)\n",
    "        # TODO Change rotation to torch operations\n",
    "        r_gt_euler_deg = dcm2euler(gt_transforms[:, :3, :3].detach().cpu().numpy(), seq='xyz')\n",
    "        r_pred_euler_deg = dcm2euler(pred_transforms[:, :3, :3].detach().cpu().numpy(), seq='xyz')\n",
    "        t_gt = gt_transforms[:, :3, 3]\n",
    "        t_pred = pred_transforms[:, :3, 3]\n",
    "        r_mse = np.mean((r_gt_euler_deg - r_pred_euler_deg) ** 2, axis=1)\n",
    "        r_mae = np.mean(np.abs(r_gt_euler_deg - r_pred_euler_deg), axis=1)\n",
    "        t_mse = torch.mean((t_gt - t_pred) ** 2, dim=1)\n",
    "        t_mae = torch.mean(torch.abs(t_gt - t_pred), dim=1)\n",
    "\n",
    "        # Rotation, translation errors (isotropic, i.e. doesn't depend on error\n",
    "        # direction, which is more representative of the actual error)\n",
    "        concatenated = se3.concatenate(se3.inverse(gt_transforms), pred_transforms)\n",
    "        rot_trace = concatenated[:, 0, 0] + concatenated[:, 1, 1] + concatenated[:, 2, 2]\n",
    "        residual_rotdeg = torch.acos(torch.clamp(0.5 * (rot_trace - 1), min=-1.0, max=1.0)) * 180.0 / np.pi\n",
    "        residual_transmag = concatenated[:, :, 3].norm(dim=-1)\n",
    "\n",
    "        # Modified Chamfer distance\n",
    "        src_transformed = se3.transform(pred_transforms, points_src)\n",
    "        ref_clean = points_raw\n",
    "        src_clean = se3.transform(se3.concatenate(pred_transforms, se3.inverse(gt_transforms)), points_raw)\n",
    "        dist_src = torch.min(square_distance(src_transformed, ref_clean), dim=-1)[0]\n",
    "        dist_ref = torch.min(square_distance(points_ref, src_clean), dim=-1)[0]\n",
    "        chamfer_dist = torch.mean(dist_src, dim=1) + torch.mean(dist_ref, dim=1)\n",
    "\n",
    "        metrics = {\n",
    "            'r_mse': r_mse,\n",
    "            'r_mae': r_mae,\n",
    "            't_mse': to_numpy(t_mse),\n",
    "            't_mae': to_numpy(t_mae),\n",
    "            'err_r_deg': to_numpy(residual_rotdeg),\n",
    "            'err_t': to_numpy(residual_transmag),\n",
    "            'chamfer_dist': to_numpy(chamfer_dist)\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def summarize_metrics(metrics):\n",
    "    \"\"\"Summaries computed metrices by taking mean over all data instances\"\"\"\n",
    "    summarized = {}\n",
    "    for k in metrics:\n",
    "        if k.endswith('mse'):\n",
    "            summarized[k[:-3] + 'rmse'] = np.sqrt(np.mean(metrics[k]))\n",
    "        elif k.startswith('err'):\n",
    "            summarized[k + '_mean'] = np.mean(metrics[k])\n",
    "            summarized[k + '_rmse'] = np.sqrt(np.mean(metrics[k]**2))\n",
    "        else:\n",
    "            summarized[k] = np.mean(metrics[k])\n",
    "\n",
    "    return summarized\n",
    "\n",
    "\n",
    "def print_metrics(logger, summary_metrics: Dict, losses_by_iteration: List = None,\n",
    "                  title: str = 'Metrics'):\n",
    "    \"\"\"Prints out formated metrics to logger\"\"\"\n",
    "\n",
    "    logger.info(title + ':')\n",
    "    logger.info('=' * (len(title) + 1))\n",
    "\n",
    "    if losses_by_iteration is not None:\n",
    "        losses_all_str = ' | '.join(['{:.5f}'.format(c) for c in losses_by_iteration])\n",
    "        logger.info('Losses by iteration: {}'.format(losses_all_str))\n",
    "\n",
    "    logger.info('DeepCP metrics:{:.4f}(rot-rmse) | {:.4f}(rot-mae) | {:.4g}(trans-rmse) | {:.4g}(trans-mae)'.format(\n",
    "        summary_metrics['r_rmse'], summary_metrics['r_mae'],\n",
    "        summary_metrics['t_rmse'], summary_metrics['t_mae'],\n",
    "    ))\n",
    "    logger.info('Rotation error {:.4f}(deg, mean) | {:.4f}(deg, rmse)'.format(\n",
    "        summary_metrics['err_r_deg_mean'], summary_metrics['err_r_deg_rmse']))\n",
    "    logger.info('Translation error {:.4g}(mean) | {:.4g}(rmse)'.format(\n",
    "        summary_metrics['err_t_mean'], summary_metrics['err_t_rmse']))\n",
    "    logger.info('Chamfer error: {:.7f}(mean-sq)'.format(\n",
    "        summary_metrics['chamfer_dist']\n",
    "    ))\n",
    "\n",
    "\n",
    "def inference(data_loader, model: torch.nn.Module):\n",
    "    \"\"\"Runs inference over entire dataset\n",
    "\n",
    "    Args:\n",
    "        data_loader (torch.utils.data.DataLoader): Dataset loader\n",
    "        model (model.nn.Module): Network model to evaluate\n",
    "\n",
    "    Returns:\n",
    "        pred_transforms_all: predicted transforms (B, n_iter, 3, 4) where B is total number of instances\n",
    "        endpoints_out (Dict): Network endpoints\n",
    "    \"\"\"\n",
    "\n",
    "    _logger.info('Starting inference...')\n",
    "    model.eval()\n",
    "\n",
    "    pred_transforms_all = []\n",
    "    all_betas, all_alphas = [], []\n",
    "    total_time = 0.0\n",
    "    endpoints_out = defaultdict(list)\n",
    "    total_rotation = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data in tqdm(data_loader):\n",
    "\n",
    "            rot_trace = val_data['transform_gt'][:, 0, 0] + val_data['transform_gt'][:, 1, 1] + \\\n",
    "                        val_data['transform_gt'][:, 2, 2]\n",
    "            rotdeg = torch.acos(torch.clamp(0.5 * (rot_trace - 1), min=-1.0, max=1.0)) * 180.0 / np.pi\n",
    "            total_rotation.append(np.abs(to_numpy(rotdeg)))\n",
    "\n",
    "            dict_all_to_device(val_data, _device)\n",
    "            time_before = time.time()\n",
    "            pred_transforms, endpoints = model(val_data, _args.num_reg_iter)\n",
    "            total_time += time.time() - time_before\n",
    "\n",
    "            if _args.method == 'rpmnet':\n",
    "                all_betas.append(endpoints['beta'])\n",
    "                all_alphas.append(endpoints['alpha'])\n",
    "\n",
    "            if isinstance(pred_transforms[-1], torch.Tensor):\n",
    "                pred_transforms_all.append(to_numpy(torch.stack(pred_transforms, dim=1)))\n",
    "            else:\n",
    "                pred_transforms_all.append(np.stack(pred_transforms, axis=1))\n",
    "\n",
    "            # Saves match matrix. We only save the top matches to save storage/time.\n",
    "            # However, this still takes quite a bit of time to save. Comment out if not needed.\n",
    "            if 'perm_matrices' in endpoints:\n",
    "                perm_matrices = to_numpy(torch.stack(endpoints['perm_matrices'], dim=1))\n",
    "                thresh = np.percentile(perm_matrices, 99.9, axis=[2, 3])  # Only retain top 0.1% of entries\n",
    "                below_thresh_mask = perm_matrices < thresh[:, :, None, None]\n",
    "                perm_matrices[below_thresh_mask] = 0.0\n",
    "\n",
    "                for i_data in range(perm_matrices.shape[0]):\n",
    "                    sparse_perm_matrices = []\n",
    "                    for i_iter in range(perm_matrices.shape[1]):\n",
    "                        sparse_perm_matrices.append(sparse.coo_matrix(perm_matrices[i_data, i_iter, :, :]))\n",
    "                    endpoints_out['perm_matrices'].append(sparse_perm_matrices)\n",
    "\n",
    "    _logger.info('Total inference time: {}s'.format(total_time))\n",
    "    total_rotation = np.concatenate(total_rotation, axis=0)\n",
    "    _logger.info('Rotation range in data: {}(avg), {}(max)'.format(np.mean(total_rotation), np.max(total_rotation)))\n",
    "    pred_transforms_all = np.concatenate(pred_transforms_all, axis=0)\n",
    "\n",
    "    return pred_transforms_all, endpoints_out\n",
    "\n",
    "\n",
    "def evaluate(pred_transforms, data_loader: torch.utils.data.dataloader.DataLoader):\n",
    "    \"\"\" Evaluates the computed transforms against the groundtruth\n",
    "\n",
    "    Args:\n",
    "        pred_transforms: Predicted transforms (B, [iter], 3/4, 4)\n",
    "        data_loader: Loader for dataset.\n",
    "\n",
    "    Returns:\n",
    "        Computed metrics (List of dicts), and summary metrics (only for last iter)\n",
    "    \"\"\"\n",
    "\n",
    "    _logger.info('Evaluating transforms...')\n",
    "    num_processed, num_total = 0, len(pred_transforms)\n",
    "\n",
    "    if pred_transforms.ndim == 4:\n",
    "        pred_transforms = torch.from_numpy(pred_transforms).to(_device)\n",
    "    else:\n",
    "        assert pred_transforms.ndim == 3 and \\\n",
    "               (pred_transforms.shape[1:] == (4, 4) or pred_transforms.shape[1:] == (3, 4))\n",
    "        pred_transforms = torch.from_numpy(pred_transforms[:, None, :, :]).to(_device)\n",
    "\n",
    "    metrics_for_iter = [defaultdict(list) for _ in range(pred_transforms.shape[1])]\n",
    "\n",
    "    for data in tqdm(data_loader, leave=False):\n",
    "        dict_all_to_device(data, _device)\n",
    "\n",
    "        batch_size = 0\n",
    "        for i_iter in range(pred_transforms.shape[1]):\n",
    "            batch_size = data['points_src'].shape[0]\n",
    "\n",
    "            cur_pred_transforms = pred_transforms[num_processed:num_processed+batch_size, i_iter, :, :]\n",
    "            metrics = compute_metrics(data, cur_pred_transforms)\n",
    "            for k in metrics:\n",
    "                metrics_for_iter[i_iter][k].append(metrics[k])\n",
    "        num_processed += batch_size\n",
    "\n",
    "    for i_iter in range(len(metrics_for_iter)):\n",
    "        metrics_for_iter[i_iter] = {k: np.concatenate(metrics_for_iter[i_iter][k], axis=0)\n",
    "                                    for k in metrics_for_iter[i_iter]}\n",
    "        summary_metrics = summarize_metrics(metrics_for_iter[i_iter])\n",
    "        print_metrics(_logger, summary_metrics, title='Evaluation result (iter {})'.format(i_iter))\n",
    "\n",
    "    return metrics_for_iter, summary_metrics\n",
    "\n",
    "\n",
    "def save_eval_data(pred_transforms, endpoints, metrics, summary_metrics, save_pathdic):\n",
    "    \"\"\"Saves out the computed transforms\n",
    "    \"\"\"\n",
    "\n",
    "    # Save transforms\n",
    "    np.save(os.path.join(save_path, 'pred_transforms.npy'), pred_transforms)\n",
    "\n",
    "    # Save endpoints if any\n",
    "    for k in endpoints:\n",
    "        if isinstance(endpoints[k], np.ndarray):\n",
    "            np.save(os.path.join(save_path, '{}.npy'.format(k)), endpoints[k])\n",
    "        else:\n",
    "            with open(os.path.join(save_path, '{}.pickle'.format(k)), 'wb') as fid:\n",
    "                pickle.dump(endpoints[k], fid)\n",
    "\n",
    "    # Save metrics: Write each iteration to a different worksheet.\n",
    "    writer = pd.ExcelWriter(os.path.join(save_path, 'metrics.xlsx'))\n",
    "    for i_iter in range(len(metrics)):\n",
    "        metrics[i_iter]['r_rmse'] = np.sqrt(metrics[i_iter]['r_mse'])\n",
    "        metrics[i_iter]['t_rmse'] = np.sqrt(metrics[i_iter]['t_mse'])\n",
    "        metrics[i_iter].pop('r_mse')\n",
    "        metrics[i_iter].pop('t_mse')\n",
    "        metrics_df = pd.DataFrame.from_dict(metrics[i_iter])\n",
    "        metrics_df.to_excel(writer, sheet_name='Iter_{}'.format(i_iter+1))\n",
    "    writer.close()\n",
    "\n",
    "    # Save summary metrics\n",
    "    summary_metrics_float = {k: float(summary_metrics[k]) for k in summary_metrics}\n",
    "    with open(os.path.join(save_path, 'summary_metrics.json'), 'w') as json_out:\n",
    "        json.dump(summary_metrics_float, json_out)\n",
    "\n",
    "    _logger.info('Saved evaluation results to {}'.format(save_path))\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    _logger.info('Computing transforms using {}'.format(_args.method))\n",
    "    if _args.method == 'rpmnet':\n",
    "        assert _args.resume is not None\n",
    "        model = models.rpmnet.get_model(_args)\n",
    "        model.to(_device)\n",
    "        saver = CheckPointManager(os.path.join(_log_path, 'ckpt', 'models'))\n",
    "        saver.load(_args.resume, model)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load data_loader\n",
    "    test_dataset = get_test_datasets(_args)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=_args.val_batch_size, shuffle=False)\n",
    "\n",
    "    if _args.transform_file is not None:\n",
    "        _logger.info('Loading from precomputed transforms: {}'.format(_args.transform_file))\n",
    "        pred_transforms = np.load(_args.transform_file)\n",
    "        endpoints = {}\n",
    "    else:\n",
    "        model = get_model()\n",
    "        pred_transforms, endpoints = inference(test_loader, model)  # Feedforward transforms\n",
    "\n",
    "    # Compute evaluation matrices\n",
    "    eval_metrics, summary_metrics = evaluate(pred_transforms, data_loader=test_loader)\n",
    "\n",
    "    save_eval_data(pred_transforms, endpoints, eval_metrics, summary_metrics, _args.eval_save_path)\n",
    "    _logger.info('Finished')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM root[10932] INFO Command: /home/li/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /run/user/1000/jupyter/kernel-cc4ba887-c871-4f2c-8e07-dc2c931eb22b.json\n",
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM root[10932] INFO Source is from Commit c37e6873 (2020-04-25): Release of source code\n",
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM root[10932] INFO Arguments: logdir: ../logs, dev: False, name: None, debug: False, dataset_path: ../datasets/modelnet40_ply_hdf5_2048, dataset_type: modelnet_hdf, num_points: 1024, noise_type: crop, rot_mag: 45.0, trans_mag: 0.5, partial: [0.7, 0.7], method: rpmnet, radius: 0.3, num_neighbors: 64, features: ['ppf', 'dxyz', 'xyz'], feat_dim: 96, no_slack: False, num_sk_iter: 5, num_reg_iter: 10, loss_type: mae, wt_inliers: 0.01, train_batch_size: 8, val_batch_size: 2, resume: ../logs/clean-trained.pth, gpu: 0, test_category_file: ./data_loader/modelnet40_half2.txt, transform_file: None, eval_save_path: ../eval_results\n",
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM root[10932] INFO Output and logs will be saved to ../eval_results\n",
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM root[10932] INFO Computing transforms using rpmnet\n",
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM ParameterPredictionNet[10932] INFO Predicting weights with dim [0].\n",
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM FeatExtractionEarlyFusion[10932] INFO Using early fusion, feature dim = 96\n",
      "2020-06-24 03:28:02 li-Lenovo-Y520-15IKBM FeatExtractionEarlyFusion[10932] INFO Feature extraction using features xyz, dxyz, ppf\n",
      "2020-06-24 03:28:04 li-Lenovo-Y520-15IKBM CheckPointManager[10932] INFO Loaded models from ../logs/clean-trained.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RPMNetEarlyFusion(\n",
       "  (weights_net): ParameterPredictionNet(\n",
       "    (prepool): Sequential(\n",
       "      (0): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
       "      (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (7): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (10): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (11): ReLU()\n",
       "      (12): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (13): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
       "      (14): ReLU()\n",
       "    )\n",
       "    (pooling): AdaptiveMaxPool1d(output_size=1)\n",
       "    (postpool): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): GroupNorm(16, 512, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (4): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=256, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (feat_extractor): FeatExtractionEarlyFusion(\n",
       "    (prepool): Sequential(\n",
       "      (0): Conv2d(10, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (7): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (8): ReLU()\n",
       "    )\n",
       "    (postpool): Sequential(\n",
       "      (0): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "      (1): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      (4): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = rpmnet_eval_arguments()\n",
    "_args = parser.parse_args(args=[\"--resume=../logs/clean-trained.pth\", \"--num_reg_iter=10\"])\n",
    "# print(args)\n",
    "_logger, _log_path = prepare_logger(_args, log_path=_args.eval_save_path)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(_args.gpu)\n",
    "if _args.gpu >= 0 and (_args.method == 'rpm' or _args.method == 'rpmnet'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(_args.gpu)\n",
    "    _device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "else:\n",
    "    _device = torch.device('cpu')\n",
    "\n",
    "model = get_model()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## official test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-24 01:13:33 li-Lenovo-Y520-15IKBM root[6798] INFO Test transforms: SetDeterministic, SplitSourceRef, RandomCrop, RandomTransformSE3_euler, Resampler, RandomJitter, ShufflePoints\n",
      "2020-06-24 01:13:33 li-Lenovo-Y520-15IKBM ModelNetHdf[6798] INFO Loading data from ../datasets/modelnet40_ply_hdf5_2048/test_files.txt for test\n",
      "2020-06-24 01:13:33 li-Lenovo-Y520-15IKBM ModelNetHdf[6798] INFO Categories used: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39].\n",
      "2020-06-24 01:13:34 li-Lenovo-Y520-15IKBM ModelNetHdf[6798] INFO Loaded 1266 test instances.\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "test_dataset = get_test_datasets(_args)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=_args.val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "val_data = ''\n",
    "for data in test_loader:\n",
    "    if i==1:\n",
    "        val_data = data\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(dict_all_to_device)\n",
    "dict_all_to_device(val_data, _device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dict_all_to_device(val_data, _device)\n",
    "    pred_transforms, endpoints = model(val_data, _args.num_reg_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 717, 6]), torch.Size([2, 717, 6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['points_ref'].shape, val_data['points_src'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load LiDAR cloud and compute transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "# pcd1 = o3d.io.read_point_cloud(\"/home/li/Documents/pcl_tutorial/room_scan1_sub.pcd\")\n",
    "# pcd2 = o3d.io.read_point_cloud(\"/home/li/Documents/pcl_tutorial/room_scan2_sub.pcd\")\n",
    "# pcd1 = o3d.io.read_point_cloud(\"/home/li/Documents/pcl_tutorial/remaining_cloud011_sub.pcd\")\n",
    "# pcd2 = o3d.io.read_point_cloud(\"/home/li/Documents/pcl_tutorial/remaining_cloud012_sub.pcd\")\n",
    "pcd1 = o3d.io.read_point_cloud(\"/home/li/car1_map_sub.pcd\")\n",
    "pcd2 = o3d.io.read_point_cloud(\"/home/li/car2_map_sub.pcd\")\n",
    "\n",
    "\n",
    "pc1 = np.asarray(pcd1.points)\n",
    "pc2 = np.asarray(pcd2.points)\n",
    "center1 = pc1.mean(axis=0)\n",
    "center2 = pc2.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.asarray(pcd1.normals)\n",
    "val_data = {}\n",
    "val_data['points_ref'] = torch.Tensor([np.concatenate(\n",
    "    [np.asarray(pcd1.points)-center1, np.asarray(pcd1.normals)], axis=1)])\n",
    "val_data['points_src'] = torch.Tensor([np.concatenate(\n",
    "    [np.asarray(pcd2.points)-center2, np.asarray(pcd2.normals)], axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3688, 6]), torch.Size([1, 3887, 6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['points_ref'].shape, val_data['points_src'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dict_all_to_device(val_data, _device)\n",
    "    pred_transforms, endpoints = model(val_data, _args.num_reg_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.9988, -0.0480, -0.0098, -1.0657],\n",
       "          [ 0.0483,  0.9983,  0.0315, -1.2318],\n",
       "          [ 0.0082, -0.0320,  0.9995, -0.1457]]], device='cuda:0'),\n",
       " tensor([[[ 9.9733e-01, -7.3063e-02,  1.6052e-03, -1.0984e+00],\n",
       "          [ 7.2903e-02,  9.9620e-01,  4.7739e-02, -1.6073e+00],\n",
       "          [-5.0870e-03, -4.7494e-02,  9.9886e-01, -2.6481e-01]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[ 0.9958, -0.0907,  0.0091, -1.1385],\n",
       "          [ 0.0901,  0.9944,  0.0561, -1.8076],\n",
       "          [-0.0141, -0.0551,  0.9984, -0.3168]]], device='cuda:0'),\n",
       " tensor([[[ 0.9945, -0.1037,  0.0134, -1.1950],\n",
       "          [ 0.1027,  0.9928,  0.0609, -1.9474],\n",
       "          [-0.0196, -0.0592,  0.9981, -0.3487]]], device='cuda:0'),\n",
       " tensor([[[ 0.9934, -0.1140,  0.0159, -1.2407],\n",
       "          [ 0.1127,  0.9916,  0.0639, -2.0446],\n",
       "          [-0.0231, -0.0617,  0.9978, -0.3709]]], device='cuda:0'),\n",
       " tensor([[[ 0.9923, -0.1223,  0.0175, -1.2687],\n",
       "          [ 0.1209,  0.9905,  0.0658, -2.1093],\n",
       "          [-0.0253, -0.0632,  0.9977, -0.3864]]], device='cuda:0'),\n",
       " tensor([[[ 0.9915, -0.1291,  0.0183, -1.2854],\n",
       "          [ 0.1276,  0.9896,  0.0668, -2.1517],\n",
       "          [-0.0268, -0.0639,  0.9976, -0.3966]]], device='cuda:0'),\n",
       " tensor([[[ 0.9907, -0.1346,  0.0189, -1.2973],\n",
       "          [ 0.1331,  0.9888,  0.0675, -2.1806],\n",
       "          [-0.0277, -0.0643,  0.9975, -0.4032]]], device='cuda:0'),\n",
       " tensor([[[ 0.9901, -0.1391,  0.0192, -1.3063],\n",
       "          [ 0.1375,  0.9882,  0.0678, -2.2012],\n",
       "          [-0.0284, -0.0645,  0.9975, -0.4077]]], device='cuda:0'),\n",
       " tensor([[[ 0.9896, -0.1428,  0.0194, -1.3135],\n",
       "          [ 0.1412,  0.9876,  0.0680, -2.2162],\n",
       "          [-0.0289, -0.0645,  0.9975, -0.4108]]], device='cuda:0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9991929531097412 -0.0336252860724926 0.021978307515382767 -1.7121531963348389 \n",
      "0.03207755833864212 0.9972147941589355 0.0673338770866394 -2.0785439014434814 \n",
      "-0.024181202054023743 -0.06657449901103973 0.9974883794784546 -0.6212092041969299 \n",
      "0 0 0 1\n"
     ]
    }
   ],
   "source": [
    "transform = pred_transforms[-1][0].tolist()\n",
    "T_ab_str = ''\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        T_ab_str += str(transform[i][j]) + ' '\n",
    "    T_ab_str += '\\n'\n",
    "T_ab_str += '0 0 0 1'\n",
    "#     print(l)\n",
    "#     T_ab_str += str(rotation_ab_pred_list[i][0]) + ' ' + str(rotation_ab_pred_list[i][1])  \\\n",
    "#     + ' ' + str(rotation_ab_pred_list[i][2]) + ' ' \\\n",
    "#     + str(translation_ab_pred_list[i]) + '\\n'\n",
    "print(T_ab_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load single point cloud and manually transform it as a source cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22437914  0.9745019   0.          0.        ]\n",
      " [-0.9745019   0.22437914  0.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import data_loader.transforms as Transforms\n",
    "import common.math.se3 as se3\n",
    "SE3_Z = Transforms.RandomRotatorZ()\n",
    "transform_mat = SE3_Z.generate_transform()\n",
    "print(transform_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "pcd = o3d.io.read_point_cloud(\"/home/li/Lille_street_small_sub.pcd\")\n",
    "pc1 = np.asarray(pcd.points)\n",
    "pc2 = se3.transform(transform_mat, pc1[:, :3])\n",
    "# pc1.shape, pc2.shape\n",
    "center1 = pc1.mean(axis=0)\n",
    "center2 = pc2.mean(axis=0)\n",
    "# center1, center2\n",
    "pc1 = pc1 - center1\n",
    "pc2 = pc2 - center2\n",
    "\n",
    "normals1 = np.asarray(pcd.normals)\n",
    "normals2 = se3.transform(transform_mat, normals1)\n",
    "# normals1, normals2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.30791559e-08,  1.22741652e-08,  1.72647062e-08]),\n",
       " array([9.02660164e-09, 1.54997566e-08, 1.72647062e-08]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center1, center2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = {}\n",
    "val_data['points_ref'] = torch.Tensor([np.concatenate(\n",
    "    [pc1, normals1], axis=1)])\n",
    "val_data['points_src'] = torch.Tensor([np.concatenate(\n",
    "    [pc2, normals2], axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3819, 6]), torch.Size([1, 3819, 6]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['points_ref'].shape, val_data['points_src'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dict_all_to_device(val_data, _device)\n",
    "    pred_transforms, endpoints = model(val_data, _args.num_reg_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.5894,  0.1467,  0.7944, -0.0415],\n",
       "          [ 0.7823,  0.1418, -0.6065,  0.0311],\n",
       "          [-0.2016,  0.9790, -0.0312, -0.0805]]], device='cuda:0'),\n",
       " tensor([[[ 0.2754,  0.3249,  0.9047,  0.1849],\n",
       "          [ 0.9526, -0.2190, -0.2113,  0.0454],\n",
       "          [ 0.1295,  0.9200, -0.3698, -0.3583]]], device='cuda:0'),\n",
       " tensor([[[ 0.3019,  0.7024,  0.6446, -0.1066],\n",
       "          [ 0.9388, -0.3368, -0.0726, -0.2666],\n",
       "          [ 0.1661,  0.6270, -0.7611, -0.3061]]], device='cuda:0'),\n",
       " tensor([[[ 0.3098,  0.8609,  0.4035, -0.1492],\n",
       "          [ 0.9352, -0.3525,  0.0341, -0.2603],\n",
       "          [ 0.1716,  0.3668, -0.9143, -0.2259]]], device='cuda:0'),\n",
       " tensor([[[ 0.3221,  0.9147,  0.2440, -0.1329],\n",
       "          [ 0.9303, -0.3536,  0.0973, -0.1719],\n",
       "          [ 0.1753,  0.1956, -0.9649, -0.0875]]], device='cuda:0'),\n",
       " tensor([[[ 0.3333,  0.9325,  0.1393, -0.1240],\n",
       "          [ 0.9256, -0.3517,  0.1397, -0.1467],\n",
       "          [ 0.1792,  0.0824, -0.9803, -0.0359]]], device='cuda:0'),\n",
       " tensor([[[ 0.3405,  0.9395,  0.0375, -0.1322],\n",
       "          [ 0.9223, -0.3415,  0.1806, -0.1628],\n",
       "          [ 0.1825, -0.0269, -0.9828, -0.0282]]], device='cuda:0'),\n",
       " tensor([[[ 0.3417,  0.9366, -0.0783, -0.1419],\n",
       "          [ 0.9207, -0.3169,  0.2277, -0.1809],\n",
       "          [ 0.1884, -0.1499, -0.9706, -0.0268]]], device='cuda:0'),\n",
       " tensor([[[ 0.3387,  0.9182, -0.2052, -0.1507],\n",
       "          [ 0.9201, -0.2777,  0.2762, -0.1966],\n",
       "          [ 0.1967, -0.2824, -0.9389, -0.0245]]], device='cuda:0'),\n",
       " tensor([[[ 0.3284,  0.8891, -0.3189, -0.1586],\n",
       "          [ 0.9221, -0.2286,  0.3122, -0.2119],\n",
       "          [ 0.2047, -0.3966, -0.8949, -0.0189]]], device='cuda:0')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3284177780151367 0.8890770673751831 -0.3188788592815399 -0.15857288241386414 \n",
      "0.9220850467681885 -0.22862330079078674 0.3122354745864868 -0.21190305054187775 \n",
      "0.20469826459884644 -0.39657703042030334 -0.8948885798454285 -0.018927641212940216 \n",
      "0 0 0 1\n"
     ]
    }
   ],
   "source": [
    "transform = pred_transforms[-1][0].tolist()\n",
    "T_ab_str = ''\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        T_ab_str += str(transform[i][j]) + ' '\n",
    "    T_ab_str += '\\n'\n",
    "T_ab_str += '0 0 0 1'\n",
    "print(T_ab_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd.points = o3d.utility.Vector3dVector(pc1)\n",
    "o3d.io.write_point_cloud(\"/home/li/Lille_street_small_sub.pcd\", pcd)\n",
    "\n",
    "pcd2 = o3d.geometry.PointCloud()\n",
    "pcd2.points = o3d.utility.Vector3dVector(pc2)\n",
    "o3d.io.write_point_cloud(\"/home/li/Lille_street_small_rotated.pcd\", pcd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train RPMNet\n",
    "\n",
    "Example usage:\n",
    "    python train.py --noise_type crop\n",
    "    python train.py --noise_type jitter --train_batch_size 4\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "from matplotlib.pyplot import cm as colormap\n",
    "import numpy as np\n",
    "import open3d  # Ensure this is imported before pytorch\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from arguments import rpmnet_train_arguments\n",
    "from common.colors import BLUE, ORANGE\n",
    "from common.misc import prepare_logger\n",
    "from common.torch import dict_all_to_device, CheckPointManager, TorchDebugger\n",
    "from common.math_torch import se3\n",
    "from data_loader.datasets import get_train_datasets\n",
    "from eval import compute_metrics, summarize_metrics, print_metrics\n",
    "from models.rpmnet import get_model\n",
    "\n",
    "# Set up arguments and logging\n",
    "parser = rpmnet_train_arguments()\n",
    "_args = parser.parse_args()\n",
    "_logger, _log_path = prepare_logger(_args)\n",
    "if _args.gpu >= 0:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(_args.gpu)\n",
    "    _device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "else:\n",
    "    _device = torch.device('cpu')\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_set, val_set = get_train_datasets(_args)\n",
    "    run(train_set, val_set)\n",
    "\n",
    "\n",
    "def compute_losses(data: Dict, pred_transforms: List, endpoints: Dict,\n",
    "                   loss_type: str = 'mae', reduction: str = 'mean') -> Dict:\n",
    "    \"\"\"Compute losses\n",
    "\n",
    "    Args:\n",
    "        data: Current mini-batch data\n",
    "        pred_transforms: Predicted transform, to compute main registration loss\n",
    "        endpoints: Endpoints for training. For computing outlier penalty\n",
    "        loss_type: Registration loss type, either 'mae' (Mean absolute error, used in paper) or 'mse'\n",
    "        reduction: Either 'mean' or 'none'. Use 'none' to accumulate losses outside\n",
    "                   (useful for accumulating losses for entire validation dataset)\n",
    "\n",
    "    Returns:\n",
    "        losses: Dict containing various fields. Total loss to be optimized is in losses['total']\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    losses = {}\n",
    "    num_iter = len(pred_transforms)\n",
    "\n",
    "    # Compute losses\n",
    "    gt_src_transformed = se3.transform(data['transform_gt'], data['points_src'][..., :3])\n",
    "    if loss_type == 'mse':\n",
    "        # MSE loss to the groundtruth (does not take into account possible symmetries)\n",
    "        criterion = nn.MSELoss(reduction=reduction)\n",
    "        for i in range(num_iter):\n",
    "            pred_src_transformed = se3.transform(pred_transforms[i], data['points_src'][..., :3])\n",
    "            if reduction.lower() == 'mean':\n",
    "                losses['mse_{}'.format(i)] = criterion(pred_src_transformed, gt_src_transformed)\n",
    "            elif reduction.lower() == 'none':\n",
    "                losses['mse_{}'.format(i)] = torch.mean(criterion(pred_src_transformed, gt_src_transformed),\n",
    "                                                        dim=[-1, -2])\n",
    "    elif loss_type == 'mae':\n",
    "        # MSE loss to the groundtruth (does not take into account possible symmetries)\n",
    "        criterion = nn.L1Loss(reduction=reduction)\n",
    "        for i in range(num_iter):\n",
    "            pred_src_transformed = se3.transform(pred_transforms[i], data['points_src'][..., :3])\n",
    "            if reduction.lower() == 'mean':\n",
    "                losses['mae_{}'.format(i)] = criterion(pred_src_transformed, gt_src_transformed)\n",
    "            elif reduction.lower() == 'none':\n",
    "                losses['mae_{}'.format(i)] = torch.mean(criterion(pred_src_transformed, gt_src_transformed),\n",
    "                                                        dim=[-1, -2])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Penalize outliers\n",
    "    for i in range(num_iter):\n",
    "        ref_outliers_strength = (1.0 - torch.sum(endpoints['perm_matrices'][i], dim=1)) * _args.wt_inliers\n",
    "        src_outliers_strength = (1.0 - torch.sum(endpoints['perm_matrices'][i], dim=2)) * _args.wt_inliers\n",
    "        if reduction.lower() == 'mean':\n",
    "            losses['outlier_{}'.format(i)] = torch.mean(ref_outliers_strength) + torch.mean(src_outliers_strength)\n",
    "        elif reduction.lower() == 'none':\n",
    "            losses['outlier_{}'.format(i)] = torch.mean(ref_outliers_strength, dim=1) + \\\n",
    "                                             torch.mean(src_outliers_strength, dim=1)\n",
    "\n",
    "    discount_factor = 0.5  # Early iterations will be discounted\n",
    "    total_losses = []\n",
    "    for k in losses:\n",
    "        discount = discount_factor ** (num_iter - int(k[k.rfind('_')+1:]) - 1)\n",
    "        total_losses.append(losses[k] * discount)\n",
    "    losses['total'] = torch.sum(torch.stack(total_losses), dim=0)\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def save_summaries(writer: SummaryWriter, data: Dict, predicted: List, endpoints: Dict = None,\n",
    "                   losses: Dict = None, metrics: Dict = None, step: int = 0):\n",
    "    \"\"\"Save tensorboard summaries\"\"\"\n",
    "\n",
    "    subset = [0, 1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Save clouds\n",
    "        if 'points_src' in data:\n",
    "\n",
    "            points_src = data['points_src'][subset, ..., :3]\n",
    "            points_ref = data['points_ref'][subset, ..., :3]\n",
    "\n",
    "            colors = torch.from_numpy(\n",
    "                np.concatenate([np.tile(ORANGE, (*points_src.shape[0:2], 1)),\n",
    "                                np.tile(BLUE, (*points_ref.shape[0:2], 1))], axis=1))\n",
    "\n",
    "            iters_to_save = [0, len(predicted)-1] if len(predicted) > 1 else [0]\n",
    "\n",
    "            # Save point cloud at iter0, iter1 and after last iter\n",
    "            concat_cloud_input = torch.cat((points_src, points_ref), dim=1)\n",
    "            writer.add_mesh('iter_0', vertices=concat_cloud_input, colors=colors, global_step=step)\n",
    "            for i_iter in iters_to_save:\n",
    "                src_transformed_first = se3.transform(predicted[i_iter][subset, ...], points_src)\n",
    "                concat_cloud_first = torch.cat((src_transformed_first, points_ref), dim=1)\n",
    "                writer.add_mesh('iter_{}'.format(i_iter+1), vertices=concat_cloud_first, colors=colors, global_step=step)\n",
    "\n",
    "            if endpoints is not None and 'perm_matrices' in endpoints:\n",
    "                color_mapper = colormap.ScalarMappable(norm=None, cmap=colormap.get_cmap('coolwarm'))\n",
    "                for i_iter in iters_to_save:\n",
    "                    ref_weights = torch.sum(endpoints['perm_matrices'][i_iter][subset, ...], dim=1)\n",
    "                    ref_colors = color_mapper.to_rgba(ref_weights.detach().cpu().numpy())[..., :3]\n",
    "                    writer.add_mesh('ref_weights_{}'.format(i_iter), vertices=points_ref,\n",
    "                                    colors=torch.from_numpy(ref_colors) * 255, global_step=step)\n",
    "\n",
    "        if endpoints is not None:\n",
    "            if 'perm_matrices' in endpoints:\n",
    "                for i_iter in range(len(endpoints['perm_matrices'])):\n",
    "                    src_weights = torch.sum(endpoints['perm_matrices'][i_iter], dim=2)\n",
    "                    ref_weights = torch.sum(endpoints['perm_matrices'][i_iter], dim=1)\n",
    "                    writer.add_histogram('src_weights_{}'.format(i_iter), src_weights, global_step=step)\n",
    "                    writer.add_histogram('ref_weights_{}'.format(i_iter), ref_weights, global_step=step)\n",
    "\n",
    "        # Write losses and metrics\n",
    "        if losses is not None:\n",
    "            for l in losses:\n",
    "                writer.add_scalar('losses/{}'.format(l), losses[l], step)\n",
    "        if metrics is not None:\n",
    "            for m in metrics:\n",
    "                writer.add_scalar('metrics/{}'.format(m), metrics[m], step)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "def validate(data_loader, model: torch.nn.Module, summary_writer: SummaryWriter, step: int):\n",
    "    \"\"\"Perform a single validation run, and saves results into tensorboard summaries\"\"\"\n",
    "\n",
    "    _logger.info('Starting validation run...')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_val_losses = defaultdict(list)\n",
    "        all_val_metrics_np = defaultdict(list)\n",
    "        for val_data in data_loader:\n",
    "            dict_all_to_device(val_data, _device)\n",
    "            pred_test_transforms, endpoints = model(val_data, _args.num_reg_iter)\n",
    "            val_losses = compute_losses(val_data, pred_test_transforms, endpoints,\n",
    "                                        loss_type=_args.loss_type, reduction='none')\n",
    "            val_metrics = compute_metrics(val_data, pred_test_transforms[-1])\n",
    "\n",
    "            for k in val_losses:\n",
    "                all_val_losses[k].append(val_losses[k])\n",
    "            for k in val_metrics:\n",
    "                all_val_metrics_np[k].append(val_metrics[k])\n",
    "\n",
    "        all_val_losses = {k: torch.cat(all_val_losses[k]) for k in all_val_losses}\n",
    "        all_val_metrics_np = {k: np.concatenate(all_val_metrics_np[k]) for k in all_val_metrics_np}\n",
    "        mean_val_losses = {k: torch.mean(all_val_losses[k]) for k in all_val_losses}\n",
    "\n",
    "    # Rerun on random and worst data instances and save to summary\n",
    "    rand_idx = random.randint(0, all_val_losses['total'].shape[0] - 1)\n",
    "    worst_idx = torch.argmax(all_val_losses['{}_{}'.format(_args.loss_type, _args.num_reg_iter - 1)]).cpu().item()\n",
    "    indices_to_rerun = [rand_idx, worst_idx]\n",
    "    data_to_rerun = defaultdict(list)\n",
    "    for i in indices_to_rerun:\n",
    "        cur = data_loader.dataset[i]\n",
    "        for k in cur:\n",
    "            data_to_rerun[k].append(cur[k])\n",
    "    for k in data_to_rerun:\n",
    "        data_to_rerun[k] = torch.from_numpy(np.stack(data_to_rerun[k], axis=0))\n",
    "    dict_all_to_device(data_to_rerun, _device)\n",
    "    pred_transforms, endpoints = model(data_to_rerun, _args.num_reg_iter)\n",
    "\n",
    "    summary_metrics = summarize_metrics(all_val_metrics_np)\n",
    "    losses_by_iteration = torch.stack([mean_val_losses['{}_{}'.format(_args.loss_type, k)]\n",
    "                                       for k in range(_args.num_reg_iter)]).cpu().numpy()\n",
    "    print_metrics(_logger, summary_metrics, losses_by_iteration, 'Validation results')\n",
    "\n",
    "    save_summaries(summary_writer, data=data_to_rerun, predicted=pred_transforms, endpoints=endpoints,\n",
    "                   losses=mean_val_losses, metrics=summary_metrics, step=step)\n",
    "\n",
    "    score = -summary_metrics['chamfer_dist']\n",
    "    return score\n",
    "\n",
    "\n",
    "def run(train_set, val_set):\n",
    "    \"\"\"Main train/val loop\"\"\"\n",
    "\n",
    "    _logger.debug('Trainer (PID=%d), %s', os.getpid(), _args)\n",
    "\n",
    "    model = get_model(_args)\n",
    "    model.to(_device)\n",
    "    global_step = 0\n",
    "\n",
    "    # dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=_args.train_batch_size, shuffle=True, num_workers=_args.num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                             batch_size=_args.val_batch_size, shuffle=False, num_workers=_args.num_workers)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=_args.lr)\n",
    "\n",
    "    # Summary writer and Checkpoint manager\n",
    "    train_writer = SummaryWriter(os.path.join(_log_path, 'train'), flush_secs=10)\n",
    "    val_writer = SummaryWriter(os.path.join(_log_path, 'val'), flush_secs=10)\n",
    "    saver = CheckPointManager(os.path.join(_log_path, 'ckpt', 'model'), keep_checkpoint_every_n_hours=0.5)\n",
    "    if _args.resume is not None:\n",
    "        global_step = saver.load(_args.resume, model, optimizer)\n",
    "\n",
    "    # trainings\n",
    "    torch.autograd.set_detect_anomaly(_args.debug)\n",
    "    model.train()\n",
    "\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    if _args.summary_every < 0:\n",
    "        _args.summary_every = abs(_args.summary_every) * steps_per_epoch\n",
    "    if _args.validate_every < 0:\n",
    "        _args.validate_every = abs(_args.validate_every) * steps_per_epoch\n",
    "\n",
    "    for epoch in range(0, _args.epochs):\n",
    "        _logger.info('Begin epoch {} (steps {} - {})'.format(epoch, global_step, global_step + len(train_loader)))\n",
    "        tbar = tqdm(total=len(train_loader), ncols=100)\n",
    "        for train_data in train_loader:\n",
    "            global_step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward through neural network\n",
    "            dict_all_to_device(train_data, _device)\n",
    "            pred_transforms, endpoints = model(train_data, _args.num_train_reg_iter)  # Use less iter during training\n",
    "\n",
    "            # Compute loss, and optimize\n",
    "            train_losses = compute_losses(train_data, pred_transforms, endpoints,\n",
    "                                          loss_type=_args.loss_type, reduction='mean')\n",
    "            if _args.debug:\n",
    "                with TorchDebugger():\n",
    "                    train_losses['total'].backward()\n",
    "            else:\n",
    "                train_losses['total'].backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tbar.set_description('Loss:{:.3g}'.format(train_losses['total']))\n",
    "            tbar.update(1)\n",
    "\n",
    "            if global_step % _args.summary_every == 0:  # Save tensorboard logs\n",
    "                save_summaries(train_writer, data=train_data, predicted=pred_transforms, endpoints=endpoints,\n",
    "                               losses=train_losses, step=global_step)\n",
    "\n",
    "            if global_step % _args.validate_every == 0:  # Validation loop. Also saves checkpoints\n",
    "                model.eval()\n",
    "                val_score = validate(val_loader, model, val_writer, global_step)\n",
    "                saver.save(model, optimizer, step=global_step, score=val_score)\n",
    "                model.train()\n",
    "\n",
    "        tbar.close()\n",
    "\n",
    "    _logger.info('Ending training. Number of steps = {}.'.format(global_step))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
